import json import numpy as np import math import statistics import matplotlib.pyplot as plt # --- Data Structures --- class Item: """Represents an object characterized by a dictionary of attributes.""" def __init__(self, attributes: dict): self.attributes = attributes def get_attribute(self, attribute_name: str): return self.attributes.get(attribute_name) def __str__(self): return f"Item({self.attributes})" class HistoricalNoveltyModel: """ A model based on Terence McKenna's Timewave Zero theory. It calculates a novelty value based on a fractal wave derived from the I Ching. Lower raw timewave values indicate higher novelty. """ def __init__(self): # The King Wen sequence (first order of differences) from the C code self.h = { 1: 6, 2: 2, 3: 4, 4: 4, 5: 4, 6: 3, 7: 2, 8: 4, 9: 2, 10: 4, 11: 6, 12: 2, 13: 2, 14: 4, 15: 2, 16: 2, 17: 6, 18: 3, 19: 4, 20: 3, 21: 2, 22: 2, 23: 2, 24: 3, 25: 4, 26: 2, 27: 6, 28: 2, 29: 6, 30: 3, 31: 2, 32: 3, 33: 4, 34: 4, 35: 4, 36: 2, 37: 4, 38: 6, 39: 4, 40: 3, 41: 2, 42: 4, 43: 2, 44: 3, 45: 4, 46: 3, 47: 2, 48: 3, 49: 4, 50: 4, 51: 4, 52: 1, 53: 6, 54: 2, 55: 2, 56: 3, 57: 4, 58: 3, 59: 2, 60: 1, 61: 6, 62: 3, 63: 6, 64: 3, 0: 3 } # The 'Watkins' dataset (no half-twist), generated from the King Wen sequence. self.WAVE_DATA = self._generate_watkins_data() self.NUM_DATA_POINTS = 384 self.WAVE_FACTOR = 64.0 self.POWERS = np.array([self.WAVE_FACTOR**i for i in range(13)], dtype=np.longdouble) def _mod_64(self, i): return i % 64 if i >= 0 else (i % 64 + 64) % 64 def _exp_minus_one(self, i): return -1 if abs(i) % 2 else 1 def _generate_watkins_data(self): w = [0] * 384 for k in range(384): a = (self._exp_minus_one((k - 1) // 32)) * \ (self.h[self._mod_64(k - 1)] - self.h[self._mod_64(k - 2)] + self.h[self._mod_64(-k)] - self.h[self._mod_64(1 - k)]) + \ 3 * (self._exp_minus_one((k - 3) // 96)) * \ (self.h[self._mod_64((k // 3) - 1)] - self.h[self._mod_64((k // 3) - 2)] + self.h[self._mod_64(-(k // 3))] - self.h[self._mod_64(1 - (k // 3))]) + \ 6 * (self._exp_minus_one((k - 6) // 192)) * \ (self.h[self._mod_64((k // 6) - 1)] - self.h[self._mod_64((k // 6) - 2)] + self.h[self._mod_64(-(k // 6))] - self.h[self._mod_64(1 - (k // 6))]) b = (9 - self.h[self._mod_64(-k)] - self.h[self._mod_64(k - 1)]) + \ 3 * (9 - self.h[self._mod_64(-(k // 3))] - self.h[self._mod_64((k // 3) - 1)]) + \ 6 * (9 - self.h[self._mod_64(-(k // 6))] - self.h[self._mod_64((k // 6) - 1)]) w[k] = abs(a) + abs(b) return w def _v(self, y): i = int(y % self.NUM_DATA_POINTS) j = (i + 1) % self.NUM_DATA_POINTS z = y - np.floor(y) return np.longdouble(self.WAVE_DATA[i]) if z == 0.0 else (np.longdouble(self.WAVE_DATA[j] - self.WAVE_DATA[i]) * z + np.longdouble(self.WAVE_DATA[i])) def get_timewave_value(self, t): if t <= 0: return np.longdouble(0.0) sum_val = np.longdouble(0.0) for i in range(len(self.POWERS)): if t < self.POWERS[i]: break sum_val += self._v(t / self.POWERS[i]) * self.POWERS[i] last_sum = np.longdouble(-1.0) i = 0 while True: i += 1 if i >= len(self.POWERS): break current_sum = sum_val sum_val += self._v(t * self.POWERS[i]) / self.POWERS[i] if sum_val <= last_sum: break last_sum = sum_val return sum_val / (self.WAVE_FACTOR**3) class Reference: def __init__(self, attribute_types: dict, historical_model: HistoricalNoveltyModel, scale_factor: float): self.items: list[Item] = [] self.attribute_types = attribute_types self._reference_stats = {} self._reference_category_counts = {} self.historical_model = historical_model self.SCALE_FACTOR = scale_factor def add_item(self, item_attributes: dict): new_item = Item(item_attributes) self.items.append(new_item) self._update_all_reference_statistics() def _update_all_reference_statistics(self): self._reference_stats = {} self._reference_category_counts = {} all_attributes_observed = set() if not self.items: return for item in self.items: all_attributes_observed.update(item.attributes.keys()) continuous_attrs = {attr for attr in all_attributes_observed if self.attribute_types.get(attr) == 'continuous'} categorical_attrs = {attr for attr in all_attributes_observed if self.attribute_types.get(attr) == 'categorical'} for attr in continuous_attrs: values = [v for item in self.items if (v := item.get_attribute(attr)) is not None and isinstance(v, (int, float))] if values: self._reference_stats[attr] = { 'mean': statistics.mean(values), 'stdev': statistics.stdev(values) if len(values) > 1 else 0 } for attr in categorical_attrs: counts = {} for item in self.items: value = item.get_attribute(attr) if value is not None: try: hash(value) counts[value] = counts.get(value, 0) + 1 except TypeError: pass if counts: self._reference_category_counts[attr] = counts def _get_attribute_novelty_score(self, item_value, attribute_name): attr_type = self.attribute_types.get(attribute_name) stats = self._reference_stats.get(attribute_name) category_counts = self._reference_category_counts.get(attribute_name) if item_value is None: return 0.0 if attr_type == 'continuous' and stats: mean, stdev = stats.get('mean', 0), stats.get('stdev', 1) if stdev == 0: return 0.0 if item_value == mean else 1.0 return 1.0 - math.exp(-((item_value - mean)**2) / (2 * stdev**2)) elif attr_type == 'categorical' and category_counts: count = category_counts.get(item_value, 0) total = sum(category_counts.values()) return 1.0 - (count / total) if total > 0 else 1.0 return 1.0 def _calculate_attribute_based_novelty(self, item: Item): if not item.attributes: return 0.0 novelty_scores = [self._get_attribute_novelty_score(val, name) for name, val in item.attributes.items()] return sum(novelty_scores) / len(novelty_scores) if novelty_scores else 0.0 def calculate_final_novelty(self, item: Item, t_conceptual: float): attribute_novelty = self._calculate_attribute_based_novelty(item) days_to_zero = (100.0 - t_conceptual) * self.SCALE_FACTOR timewave_val = self.historical_model.get_timewave_value(days_to_zero) historical_multiplier = 1.0 / (timewave_val + 1e-20) normalized_multiplier = 1 + np.log1p(historical_multiplier) return attribute_novelty * normalized_multiplier def predict_singularity(self, singularity_item: Item, start_time: float, end_time: float = 100.0, time_step: float = 0.0001): times = np.arange(start_time, end_time, time_step) novelties = [self.calculate_final_novelty(singularity_item, t) for t in times] if len(novelties) < 3: return None, [], [] first_derivative = np.gradient(novelties, time_step) second_derivative = np.gradient(first_derivative, time_step) max_accel_index = np.argmax(second_derivative) predicted_time = times[max_accel_index] return predicted_time, novelties, times # --- Main Execution Logic --- # 1. Load Data and Initialize Models try: with open("novelty_1.json", 'r') as f: epoch_data = json.load(f) except FileNotFoundError: print("Error: novelty_1.json not found. Please ensure the file is in the correct directory.") epoch_data = {"epochs":[]} timewave_model = HistoricalNoveltyModel() # 2. Set up the Reference Frame SCALE_FACTOR = 365 * 25 attribute_types = { 'length': 'continuous', 'color': 'categorical', 'weight': 'continuous', 'shape': 'categorical', 'intelligence_level': 'categorical', 'interconnectivity': 'categorical' } reference = Reference(attribute_types, timewave_model, SCALE_FACTOR) # 3. Establish the "Present Day" Context reference.add_item({'length': 1000, 'color': 'black', 'weight': 2000.0, 'shape': 'algorithm'}) reference.add_item({'length': 5000, 'color': 'blue', 'weight': 10000.0, 'shape': 'network'}) reference.add_item({'length': 1200, 'color': 'white', 'weight': 2500.0, 'shape': 'transformer_model'}) # 4. Define the Singularity Event Item singularity_item = Item({ 'length': 1000000.0, 'color': 'iridescent_spectrum', 'weight': 1e+18, 'shape': 'spacetime_fabric_manipulation', 'intelligence_level': 'superhuman_general', 'interconnectivity': 'universal_consciousness_network' }) # 5. Run Prediction and Plot Results PRESENT_DAY_CUTOFF = 99.9 TIME_STEP = 0.0001 predicted_time, novelties, times = reference.predict_singularity(singularity_item, start_time=PRESENT_DAY_CUTOFF, time_step=TIME_STEP) if predicted_time is not None: print(f"--- Singularity Prediction Result ---") print(f"Based on data up to conceptual time T={PRESENT_DAY_CUTOFF}") print(f"Predicted Singularity (point of max novelty acceleration): T={predicted_time:.8f}") # Plotting the results plt.figure(figsize=(14, 8)) plt.plot(times, novelties, label='Calculated Novelty of Singularity Event', color='dodgerblue') plt.axvline(x=predicted_time, color='red', linestyle='--', label=f'Predicted Singularity at T={predicted_time:.8f}') plt.yscale('log') plt.title('Blended Novelty Model: Predicting the Singularity', fontsize=16) plt.xlabel('Conceptual Time (Approaching 100.0)', fontsize=12) plt.ylabel('Final Novelty Score (Log Scale)', fontsize=12) plt.legend() plt.grid(True, which="both", ls="--", alpha=0.7) plt.show() else: print("Could not predict a singularity within the given time frame.")